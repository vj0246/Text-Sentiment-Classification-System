{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc168199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b0d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\vivaa\\OneDrive\\Desktop\\Sentiment_Analysis\\Dataset\\IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384466fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fd8353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c9a24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd0319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34685988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cce60b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b40f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text: str) -> str:\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text(separator=\" \", strip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "071ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f735eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,time\n",
    "string.punctuation#-->gives a kinda list for all punctuations\n",
    "exclude = string.punctuation\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3662d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "\n",
    "    # Macroeconomics\n",
    "    \"gdp\": \"gross domestic product\",\n",
    "    \"cpi\": \"consumer price index\",\n",
    "    \"ppi\": \"producer price index\",\n",
    "    \"wpi\": \"wholesale price index\",\n",
    "    \"infl\": \"inflation\",\n",
    "    \"defl\": \"deflation\",\n",
    "    \"yoy\": \"year over year\",\n",
    "    \"qoq\": \"quarter over quarter\",\n",
    "    \"mom\": \"month over month\",\n",
    "    \"ltr\": \"long term rate\",\n",
    "    \"str\": \"short term rate\",\n",
    "\n",
    "    # Central banks & policy\n",
    "    \"fed\": \"federal reserve\",\n",
    "    \"ecb\": \"european central bank\",\n",
    "    \"boe\": \"bank of england\",\n",
    "    \"boj\": \"bank of japan\",\n",
    "    \"rbi\": \"reserve bank of india\",\n",
    "    \"pbo\": \"people's bank of china\",\n",
    "    \"repo\": \"repurchase rate\",\n",
    "    \"rrr\": \"reserve requirement ratio\",\n",
    "    \"qt\": \"quantitative tightening\",\n",
    "    \"qe\": \"quantitative easing\",\n",
    "\n",
    "    # Markets & trading\n",
    "    \"ipo\": \"initial public offering\",\n",
    "    \"fpo\": \"follow on public offering\",\n",
    "    \"etf\": \"exchange traded fund\",\n",
    "    \"nav\": \"net asset value\",\n",
    "    \"ath\": \"all time high\",\n",
    "    \"atl\": \"all time low\",\n",
    "    \"vol\": \"volatility\",\n",
    "    \"oi\": \"open interest\",\n",
    "    \"deriv\": \"derivative\",\n",
    "    \"spot\": \"spot price\",\n",
    "    \"fut\": \"futures\",\n",
    "    \"opt\": \"options\",\n",
    "\n",
    "    # Corporate finance\n",
    "    \"eps\": \"earnings per share\",\n",
    "    \"pe\": \"price to earnings\",\n",
    "    \"pb\": \"price to book\",\n",
    "    \"roe\": \"return on equity\",\n",
    "    \"roa\": \"return on assets\",\n",
    "    \"ebit\": \"earnings before interest and taxes\",\n",
    "    \"ebitda\": \"earnings before interest taxes depreciation and amortization\",\n",
    "    \"fcf\": \"free cash flow\",\n",
    "    \"capex\": \"capital expenditure\",\n",
    "    \"opex\": \"operating expenditure\",\n",
    "    \"rev\": \"revenue\",\n",
    "    \"ni\": \"net income\",\n",
    "    \"gm\": \"gross margin\",\n",
    "    \"nm\": \"net margin\",\n",
    "\n",
    "    # Banking & credit\n",
    "    \"npa\": \"non performing asset\",\n",
    "    \"gnpa\": \"gross non performing asset\",\n",
    "    \"nnpa\": \"net non performing asset\",\n",
    "    \"car\": \"capital adequacy ratio\",\n",
    "    \"ltr\": \"loan to value ratio\",\n",
    "    \"ltv\": \"loan to value\",\n",
    "    \"emi\": \"equated monthly installment\",\n",
    "    \"apr\": \"annual percentage rate\",\n",
    "    \"npl\": \"non performing loan\",\n",
    "\n",
    "    # Risk & regulation\n",
    "    \"var\": \"value at risk\",\n",
    "    \"esg\": \"environmental social governance\",\n",
    "    \"aml\": \"anti money laundering\",\n",
    "    \"kyc\": \"know your customer\",\n",
    "    \"basel\": \"basel regulatory framework\",\n",
    "    \"ifrs\": \"international financial reporting standards\",\n",
    "    \"gaap\": \"generally accepted accounting principles\",\n",
    "\n",
    "    # Currencies\n",
    "    \"usd\": \"us dollar\",\n",
    "    \"eur\": \"euro\",\n",
    "    \"gbp\": \"british pound\",\n",
    "    \"jpy\": \"japanese yen\",\n",
    "    \"inr\": \"indian rupee\",\n",
    "    \"cny\": \"chinese yuan\",\n",
    "\n",
    "    # Commodities\n",
    "    \"wti\": \"west texas intermediate\",\n",
    "    \"brent\": \"brent crude oil\",\n",
    "    \"lng\": \"liquefied natural gas\",\n",
    "    \"gold\": \"gold\",\n",
    "    \"silver\": \"silver\",\n",
    "\n",
    "    # News shorthand\n",
    "    \"est\": \"estimate\",\n",
    "    \"adj\": \"adjusted\",\n",
    "    \"guid\": \"guidance\",\n",
    "    \"cons\": \"consensus\",\n",
    "    \"blk\": \"block deal\",\n",
    "    \"stk\": \"stock\",\n",
    "    \"mkt\": \"market\",\n",
    "    \"wk\": \"week\",\n",
    "    \"yr\": \"year\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d271730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.lower() in chat_words:\n",
    "            new_text.append(chat_words[w.lower()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)#-->to convert all short forms like imho,gn wagera to their full forms,\" \" join means sab ko join kar with \" \" in between each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d69620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob#-->to correct spelling mistakes\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "\n",
    "textBlb.correct().string\n",
    "incorrect_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2291b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "099c0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords#--> Stop word means all the supporting words like in, the, and, etc\n",
    "#def remove_stopwords(text):\n",
    "#    new_text = []\n",
    "#    \n",
    "#    for word in text.split():\n",
    "#        if word in stopwords.words('english'):\n",
    "#            new_text.append('')\n",
    "#        else:\n",
    "#            new_text.append(word)\n",
    "#    x = new_text[:]\n",
    "#    new_text.clear()\n",
    "#    return \" \".join(x)\n",
    "#df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "448ad59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "X_corrected = []\n",
    "\n",
    "for i in df['review']:\n",
    "    i = i.strip().lower()\n",
    "    #text_blob = TextBlob(i)\n",
    "    #i = text_blob.correct().string\n",
    "    X_corrected.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52e5163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=X_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c2a168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)\n",
    "df['review']=df['review'].apply(remove_url)\n",
    "df['review']=df['review'].apply(remove_punc1)\n",
    "df['review']=df['review'].apply(remove_emoji)\n",
    "df['review']=df['review'].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c0d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production the filming tech...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically theres a family where a little boy j...\n",
       "4        petter matteis love in the time of money is a ...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    im going to have to disagree with the previous...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 49582, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5057f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['review']\n",
    "y=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98757053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e7578b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder as LE\n",
    "le=LE()\n",
    "\n",
    "y_train=le.fit_transform(y_train)\n",
    "y_test=le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9f5a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer=Tokenizer(oov_token=\"<OOV>\",num_words=10000)#num_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f1a896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_tk=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tk=tokenizer.texts_to_sequences(X_test)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15809e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171.   277.   445.   582.   892.72]\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(seq) for seq in X_train_tk]\n",
    "\n",
    "import numpy as np\n",
    "print(np.percentile(lengths, [50, 75, 90, 95, 99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05b46c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "X_train_seq = np.array(pad_sequences(X_train_tk,padding='post',maxlen=584))\n",
    "X_test_seq=np.array(pad_sequences(X_test_tk,padding='post',maxlen=584))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b96827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=2000)#2000..\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "X_train_tfidf = X_train_tfidf.astype(np.float32)\n",
    "X_test_tfidf = X_test_tfidf.astype(np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "940416b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# Initialize Logistic Regression\\nlog_reg = LogisticRegression(\\n    random_state=42,\\n    max_iter=1000,\\n    solver=\"liblinear\"  # stable for TF-IDF + small/medium datasets\\n)\\n\\n# Hyperparameter space\\nlog_reg_params = {\\n    \"C\": [0.1, 1, 10],\\n    \"penalty\": [\"l1\", \"l2\"]\\n}\\n\\n# Randomized Search\\nrandom_logreg = RandomizedSearchCV(\\n    estimator=log_reg,\\n    param_distributions=log_reg_params,\\n    n_iter=4,\\n    cv=3,\\n    verbose=2,\\n    n_jobs=-1,\\n    scoring=\"accuracy\"\\n)\\n\\n# Fit\\nrandom_logreg.fit(X_train_tfidf, y_train)\\n\\n# Best model\\nbest_logreg = random_logreg.best_estimator_\\n\\nprint(\"Best Parameters:\", random_logreg.best_params_)\\nprint(\"Best CV Accuracy:\", random_logreg.best_score_)\\n\\n# Test set evaluation\\ny_pred = random_logreg.predict(X_test_tfidf)\\n\\nscore = accuracy_score(y_test, y_pred)\\nprint(\"Accuracy:\", score)\\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "log_reg = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver=\"liblinear\"  # stable for TF-IDF + small/medium datasets\n",
    ")\n",
    "\n",
    "# Hyperparameter space\n",
    "log_reg_params = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"penalty\": [\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_logreg = RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=log_reg_params,\n",
    "    n_iter=4,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model\n",
    "best_logreg = random_logreg.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", random_logreg.best_params_)\n",
    "print(\"Best CV Accuracy:\", random_logreg.best_score_)\n",
    "\n",
    "# Test set evaluation\n",
    "y_pred = random_logreg.predict(X_test_tfidf)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7599c043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.svm import LinearSVC\\nfrom sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\nsvm = LinearSVC(random_state=42)\\n\\nsvm_params = {\\n    \"C\": [0.1, 1, 10],\\n    #\"kernel\": [\"linear\", \"rbf\"],\\n    #\"gamma\": [\"scale\", 0.01]\\n}\\n\\n\\nrandom_svm = RandomizedSearchCV(\\n    estimator=svm,\\n    param_distributions=svm_params,\\n    n_iter=4,\\n    cv=3,\\n    verbose=2,\\n    n_jobs=-1,\\n    scoring=\"accuracy\"\\n)\\nrandom_svm.fit(X_train_tfidf, y_train)\\n\\nfrom sklearn.model_selection import cross_val_score\\n\\n#val_scores = cross_val_score(\\n#    random_svm.best_estimator_,\\n#    X_train_tfidf,\\n#    y_train,\\n#    cv=5,\\n#    scoring=\"accuracy\"\\n#)\\n#print(\"Validation Accuracy (CV Mean):\", val_scores.mean())\\n#print(\"Validation Accuracy (CV Std):\", val_scores.std())\\n\\nbest_svm = random_svm.best_estimator_\\n\\nprint(\"Best Parameters:\", random_svm.best_params_)\\nprint(\"Best CV Accuracy:\", random_svm.best_score_)\\n\\ny_pred = random_svm.predict(X_test_tfidf)\\n\\nscore = accuracy_score(y_test, y_pred)\\nprint(\"Accuracy:\", score)\\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "svm = LinearSVC(random_state=42)\n",
    "\n",
    "svm_params = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    #\"kernel\": [\"linear\", \"rbf\"],\n",
    "    #\"gamma\": [\"scale\", 0.01]\n",
    "}\n",
    "\n",
    "\n",
    "random_svm = RandomizedSearchCV(\n",
    "    estimator=svm,\n",
    "    param_distributions=svm_params,\n",
    "    n_iter=4,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "random_svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#val_scores = cross_val_score(\n",
    "#    random_svm.best_estimator_,\n",
    "#    X_train_tfidf,\n",
    "#    y_train,\n",
    "#    cv=5,\n",
    "#    scoring=\"accuracy\"\n",
    "#)\n",
    "#print(\"Validation Accuracy (CV Mean):\", val_scores.mean())\n",
    "#print(\"Validation Accuracy (CV Std):\", val_scores.std())\n",
    "\n",
    "best_svm = random_svm.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", random_svm.best_params_)\n",
    "print(\"Best CV Accuracy:\", random_svm.best_score_)\n",
    "\n",
    "y_pred = random_svm.predict(X_test_tfidf)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56cc9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Best Parameters: {'subsample': 1.0, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n",
      "Best CV Accuracy: 0.8592966553224531\n",
      "Accuracy: 0.8615508722395886\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86      4859\n",
      "           1       0.85      0.88      0.87      5058\n",
      "\n",
      "    accuracy                           0.86      9917\n",
      "   macro avg       0.86      0.86      0.86      9917\n",
      "weighted avg       0.86      0.86      0.86      9917\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4074  785]\n",
      " [ 588 4470]]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize XGBoost classifier\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",   # use \"multi:softprob\" if multiclass\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "# Hyperparameter space\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Randomized Search\n",
    "random_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=4,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_xgb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best model\n",
    "best_xgb = random_xgb.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", random_xgb.best_params_)\n",
    "print(\"Best CV Accuracy:\", random_xgb.best_score_)\n",
    "\n",
    "# Test set evaluation\n",
    "y_pred = random_xgb.predict(X_test_tfidf)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", score)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#with open(\"SVM_final.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(random_svm, file)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = len(tokenizer.word_index) + 1\n",
    "#embedding_dim = 100\n",
    "#\n",
    "#embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "#\n",
    "#for word, idx in tokenizer.word_index.items():\n",
    "#    if word in w2v_model.wv:\n",
    "#        embedding_matrix[idx] = w2v_model.wv[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b40b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147866"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=128,\n",
    "        trainable=False,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='softmax')\n",
    "]);\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e3c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "#from tensorflow.keras.layers import Embedding,SimpleRNN\n",
    "#model = Sequential()\n",
    "#model.add(Embedding(input_dim=10000,output_dim=2,50))\n",
    "#model.add(SimpleRNN(32,return_sequences=False))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d32cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 89ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 92ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 91ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 93ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 94ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 93ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 93ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 115ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 130ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 131ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 129ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 128ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 131ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 124ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 127ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 137ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 135ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 133ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 134ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "\u001b[1m1984/1984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 134ms/step - accuracy: 0.5029 - loss: 0.0000e+00 - val_accuracy: 0.4996 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_seq,y_train,epochs=20,batch_size=16,validation_split=0.2,verbose=1)#batch size=8/16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.device('/CPU:0'):\n",
    "#    model.fit(...)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3974df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Learning Accuracy: 0.500453770160675\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_seq, y_test, verbose=0)\n",
    "\n",
    "print(\"Deep Learning Accuracy:\", test_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPUs available: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPUs available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6dbe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n",
      "OrderedDict({'is_cuda_build': False, 'is_rocm_build': False, 'is_tensorrt_build': False, 'msvcp_dll_names': 'msvcp140.dll,msvcp140_1.dll'})\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.sysconfig.get_build_info())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
